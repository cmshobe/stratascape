{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter inference\n",
    "============"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "This example illustrates parameter inference for a single model.\n",
    "(Check also the `model selection <quickstart.ipynb>`_ example if you're interested\n",
    "in comparing multiple models.)\n",
    "\n",
    "\n",
    "This notebook can be downloaded here:\n",
    ":download:`Parameter Inference <parameter_inference.ipynb>`.\n",
    "\n",
    "We're going to use the following classes from the pyABC package:\n",
    "\n",
    "* :class:`ABCSMC <pyabc.smc.ABCSMC>`,\n",
    "  our entry point to parameter inference,\n",
    "* :class:`RV <pyabc.random_variables.RV>`,\n",
    "  to define the prior over a single parameter,\n",
    "* :class:`Distribution <pyabc.random_variables.Distribution>`,\n",
    "  to define the prior over a possibly higher dimensional parameter space,\n",
    "* :class:`MultivariateNormalTransition <pyabc.transition.MultivariateNormalTransition>`,\n",
    "  to do a kernel density estimate (KDE) for visualization purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Let's start to import the necessary classes. We also set up matplotlib and we're going to use pandas as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyabc\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import tempfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###my code\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import xsimlab as xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the xsimlab model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@xs.process\n",
    "class UniformGrid1D:\n",
    "    \"\"\"Create 1D model grid with uniform spacing\"\"\"\n",
    "    \n",
    "    #grid parameters\n",
    "    spacing = xs.variable(description=\"grid_spacing\", static=True)\n",
    "    length = xs.variable(description=\"grid total length\", static=True)\n",
    "    \n",
    "    #x is an index variable, used for accessing the grid.\n",
    "    x = xs.index(dims=\"x\")\n",
    "            \n",
    "    #create the grid\n",
    "    def initialize(self):\n",
    "        self.x = np.arange(0, self.length, self.spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@xs.process\n",
    "class ProfileZ:\n",
    "    \"\"\"Compute the evolution of the elevation (z) profile\"\"\"\n",
    "    \n",
    "    h_vars = xs.group(\"h_vars\") #allows for multiple processes influencing; say diffusion and subsidence\n",
    "    #br_vars = xs.group(\"br_vars\") #allows for multiple processes influencing; say diffusion and subsidence\n",
    "\n",
    "    z = xs.variable(\n",
    "        dims=\"x\", intent=\"inout\", description=\"surface elevation z\", attrs={\"units\": \"m\"}\n",
    "    )\n",
    "    #br = xs.variable(\n",
    "    #    dims=[(), \"x\"], intent=\"in\", description=\"bedrock_elevation\", attrs={\"units\": \"m\"}\n",
    "    #)\n",
    "    br = xs.variable(\n",
    "        dims=[(), \"x\"], intent=\"in\", description=\"bedrock_elevation\", attrs={\"units\": \"m\"}\n",
    "    )\n",
    "    h = xs.variable(\n",
    "        dims=\"x\", intent=\"inout\", description=\"sed_thickness\", attrs={\"units\": \"m\"}\n",
    "    )\n",
    "\n",
    "    def run_step(self):\n",
    "        #self._delta_br = sum((br for br in self.br_vars))\n",
    "        self._delta_h = sum((h for h in self.h_vars))\n",
    "\n",
    "    def finalize_step(self):\n",
    "        #self.br += self._delta_br #update bedrock surface\n",
    "        self.h += self._delta_h #update sediment thickness\n",
    "        self.z = self.br + self.h #add sediment to bedrock to get topo elev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@xs.process\n",
    "class ErosionDeposition:\n",
    "    \"\"\"Here's where the actual computation happens: nonlocal diffusion\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #get input parameters\n",
    "    basin_width = xs.variable(description=\"basin width\")\n",
    "    sea_level = xs.variable(description=\"sea level\")\n",
    "    k_factor = xs.variable(description=\"k factor\")\n",
    "    k_depth_scale = xs.variable(description=\"k depth scale\")\n",
    "    travel_dist = xs.variable(description=\"marine_travel_distance\")\n",
    "    s_crit = xs.variable(description=\"marine_critical_slope\")\n",
    "    sed_porosity = xs.variable(description=\"marine_sediment_porosity\")\n",
    "    sed_porosity_depth_scale = xs.variable(description=\"marine_sediment_porosity_depth_scale\")\n",
    "    qs_in = xs.variable(description=\"sediment_flux_in\")\n",
    "    \n",
    "    slope = xs.variable(\n",
    "        dims=\"x\", intent=\"out\", description=\"topographic_slope\", attrs={\"units\": \"-\"}\n",
    "    )\n",
    "    depth = xs.variable(\n",
    "        dims=\"x\", intent=\"out\", description=\"depth\", attrs={\"units\": \"m\"}\n",
    "    )\n",
    "    erosion = xs.variable(\n",
    "        dims=\"x\", intent=\"out\", description=\"erosion\", attrs={\"units\": \"m/yr\"}\n",
    "    )\n",
    "    deposition = xs.variable(\n",
    "        dims=\"x\", intent=\"out\", description=\"deposition\", attrs={\"units\": \"m/yr\"}\n",
    "    )\n",
    "    dh_dt = xs.variable(\n",
    "        dims=\"x\", intent=\"out\", description=\"dh_dt\", attrs={\"units\": \"m/yr\"}\n",
    "    )\n",
    "    qs = xs.variable(\n",
    "        dims=\"x\", intent=\"out\", description=\"qs\", attrs={\"units\": \"m2/yr\"}\n",
    "    )\n",
    "    #dbr = xs.variable(dims=\"x\", intent=\"out\", groups=\"br_vars\")\n",
    "    dh = xs.variable(dims=\"x\", intent=\"out\", groups=\"h_vars\")\n",
    "    \n",
    "    spacing = xs.foreign(UniformGrid1D, \"spacing\")\n",
    "    #x = xs.foreign(UniformGrid1D, \"x\")\n",
    "    z = xs.foreign(ProfileZ, \"z\")\n",
    "    br = xs.foreign(ProfileZ, \"br\")\n",
    "    h = xs.foreign(ProfileZ, \"h\")\n",
    "    \n",
    "    @xs.runtime(args=\"step_delta\")\n",
    "    def run_step(self, dt):\n",
    "        k_factor_real = np.power(10, self.k_factor)\n",
    "        self.erosion = np.repeat(0., len(self.z))\n",
    "        self.deposition = np.repeat(0., len(self.z))\n",
    "        self.dh_dt = np.repeat(0., len(self.z))\n",
    "        self.qs = np.repeat(0., len(self.z))\n",
    "        self.dh = np.repeat(0., len(self.z))\n",
    "        \n",
    "        #divide Qs_in by basin width to get qs_in\n",
    "        qs_in = self.qs_in / self.basin_width\n",
    "        \n",
    "        #calculate topographic slope\n",
    "        self.slope = np.append(np.diff(self.z) / self.spacing, 0)\n",
    "        #calculate depth below water\n",
    "        self.depth = np.maximum(self.sea_level - self.z, 0)\n",
    "        \n",
    "        #calculate k array\n",
    "        k_arr = k_factor_real * np.exp(-self.depth / self.k_depth_scale)\n",
    "        \n",
    "        #impose hard basin floor\n",
    "        k_arr[self.z <= (self.br + 0.001)] = 0\n",
    "        \n",
    "        #find first marine node\n",
    "        marine_or_terrestrial = self.z <= self.sea_level #boolean: true is marine\n",
    "        first_marine_node = np.argmax(marine_or_terrestrial) #finds the first true\n",
    "        \n",
    "        #evolve first marine node\n",
    "        if self.slope[first_marine_node] <= 0: #this is the \"regular,\" right-draining case\n",
    "            self.erosion[first_marine_node] = k_arr[first_marine_node] * np.abs(self.slope[first_marine_node])\n",
    "            self.deposition[first_marine_node] = (qs_in * (1 - np.minimum(1, np.power(self.slope[first_marine_node] / self.s_crit, 2)))) / self.travel_dist\n",
    "\n",
    "        else:  #this is the irregular, left-draining case\n",
    "            self.erosion[first_marine_node] = 0 #because slope = 0\n",
    "            self.deposition[first_marine_node] = qs_in / self.spacing #(self.qs_in * (1 + np.minimum(1, np.power(self.slope[first_marine_node] / self.s_crit, 2)))) / self.travel_dist #because slope = 0 #self.qs_in / self.travel_dist\n",
    "        self.dh_dt[first_marine_node] = (-self.erosion[first_marine_node] + self.deposition[first_marine_node]) / (1 - self.sed_porosity)\n",
    "        self.dh[first_marine_node] = self.dh_dt[first_marine_node] * dt\n",
    "        self.qs[first_marine_node] = np.maximum(qs_in + (self.erosion[first_marine_node] - self.deposition[first_marine_node]) * self.spacing, 0.)\n",
    "        if -self.dh[first_marine_node] > self.h[first_marine_node]:\n",
    "            self.dh[first_marine_node] = -self.h[first_marine_node]\n",
    "            self.qs[first_marine_node] = np.maximum(qs_in + ((-self.dh[first_marine_node] / dt) * (1 - self.sed_porosity)) * self.spacing, 0.)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #evolve remaining marine nodes\n",
    "        for i in range(first_marine_node + 1, len(self.z)): #iterate over each element of x ONLY IN THE MARINE\n",
    "        \n",
    "            if self.slope[i] <= 0: #this is the \"regular,\" right-draining case\n",
    "                self.deposition[i] = (self.qs[i-1] * (1 - np.minimum(1, np.power(self.slope[i] / self.s_crit, 2)))) / self.travel_dist\n",
    "                self.erosion[i] = k_arr[i] * np.abs(self.slope[i])\n",
    "                if self.z[i] > self.sea_level:\n",
    "                    self.deposition[i] = 0\n",
    "            else: #this is the irregular, left-draining case\n",
    "                self.deposition[i] = self.qs[i-1] / self.spacing#(self.qs[i-1] * (1 + np.minimum(1, np.power(self.slope[i] / self.s_crit, 2)))) / self.spacing#self.travel_dist #self.qs[i-1] / self.spacing\n",
    "                self.erosion[i] = 0\n",
    "                if self.z[i] > self.sea_level:\n",
    "                    self.deposition[i] = 0\n",
    "            self.dh_dt[i] = (-self.erosion[i] + self.deposition[i]) / (1 - self.sed_porosity)\n",
    "            \n",
    "            self.dh[i] = self.dh_dt[i] * dt\n",
    "            \n",
    "            self.qs[i] = np.maximum(self.qs[i-1] + (self.erosion[i] - self.deposition[i]) * self.spacing, 0.)\n",
    "            \n",
    "            if -self.dh[i] > self.h[i]:\n",
    "                self.dh[i] = -self.h[i]\n",
    "                self.qs[i] = np.maximum(self.qs[i-1] + ((-self.dh[i] / dt) * (1 - self.sed_porosity)) * self.spacing, 0.)\n",
    "            \n",
    "        #calculate change in sed thickness\n",
    "        #self.dh[:first_marine_node] = 0\n",
    "        #self.dh[first_marine_node:] = self.dh_dt[first_marine_node:] * dt\n",
    "        #self.dh[self.erosion > self.h] = -self.h[self.erosion > self.h] #if erosion is greater than h, topo only loses h    \n",
    "        \n",
    "        #compact sediment\n",
    "        #calculate the thickness after compaction, z0; dh is the thickness of new deposited solid sediment\n",
    "        dh_compact = self.dh * (1 - self.sed_porosity)\n",
    "    \n",
    "        \n",
    "        #compaction routine\n",
    "        #def compaction(porosity, porosity_depth_scale, nn, dh, zi):\n",
    "        nn = len(self.z)\n",
    "        z0 = np.zeros(nn)\n",
    "        #set initial guess for z0:\n",
    "        z0[:] = self.h[:]\n",
    "\n",
    "        #Newton-Raphson iteration  at every node\n",
    "        for k in range(nn):\n",
    "            fx = 1 #throwaway initial value to trigger \"while\" loop\n",
    "            dfx = 1 #throwaway initial value to trigger \"while\" loop\n",
    "            #check whether we're at the root\n",
    "            if dh_compact[k] > 0 : #only apply compaction where deposition is happening\n",
    "                while np.abs(fx / dfx) > 1e-6:\n",
    "                    #calculate value of function at initial guess\n",
    "                    fx = z0[k] - self.h[k] + self.sed_porosity * self.sed_porosity_depth_scale * (np.exp(-z0[k] / self.sed_porosity_depth_scale) - np.exp(-self.h[k] / self.sed_porosity_depth_scale)) - dh_compact[k]\n",
    "        \n",
    "                    #calculate derivative\n",
    "                    dfx = 1 - self.sed_porosity * np.exp(-z0[k] / self.sed_porosity_depth_scale)\n",
    "                    z0[k] = z0[k] - fx / dfx\n",
    "\n",
    "            elif dh_compact[k] == 0: #no e or d\n",
    "                z0[k] = self.h[k]\n",
    "            else: #in the case where erosion happens, the sediment surface shouldn't rebound. \n",
    "                z0[k] = self.h[k] + dh_compact[k] / (1 - self.sed_porosity)\n",
    "        \n",
    "        #here, have a chance to set the final dh by differencing new h (z0) and old h (h)\n",
    "        self.dh[:] = z0[:] - self.h[:]\n",
    "        \n",
    "        #finalize changes to bedrock (subsidence) and sediment thickness (e/d)\n",
    "        #self.dbr = (self.subsidence * dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@xs.process\n",
    "class InitBasinGeom:\n",
    "    \"\"\"\n",
    "    Give initial basin elevation field as a function of x:\n",
    "    z = exp(- (x - a) / b) + c\n",
    "    \"\"\"\n",
    "    \n",
    "    #a = xs.variable(description=\"shift parameter\", static=True)\n",
    "    #b = xs.variable(description=\"scale parameter\", static=True)\n",
    "    #c = xs.variable(description=\"initial basin floor altitude\", static=True)\n",
    "    #d = xs.variable(description=\"x multiplier\", static=True)\n",
    "\n",
    "    init_br = xs.variable(dims=\"x\", description=\"shift parameter\", static=True)\n",
    "    \n",
    "    x = xs.foreign(UniformGrid1D, \"x\")\n",
    "    z = xs.foreign(ProfileZ, \"z\", intent=\"out\")\n",
    "    #br = xs.foreign(ProfileZ, \"br\", intent=\"in\")\n",
    "    h = xs.foreign(ProfileZ, \"h\", intent=\"out\")\n",
    "    \n",
    "    #k_factor = xs.foreign(ErosionDeposition, \"k_factor\")\n",
    "    #k_depth_scale = xs.variable(description=\"k depth scale\")\n",
    "    #travel_dist = xs.variable(description=\"marine_travel_distance\")\n",
    "    #s_crit = xs.variable(description=\"marine_critical_slope\")\n",
    "\n",
    "    def initialize(self):\n",
    "        #self.br = np.exp(- (self.x * self.d - self.a) / self.b) + self.c #build the initial topography\n",
    "        self.h = np.zeros(len(self.x)) #initial sediment thickness is 0\n",
    "        self.z = np.zeros(len(self.x)) + self.init_br #self.br#(np.exp(- (self.x * self.d - self.a) / self.b) + self.c) + self.h\n",
    "        #travel_dist = parameter['erode__travel_dist']\n",
    "        #k_factor = parameter['erode__k_factor']\n",
    "        #k_depth = parameter['erode__k_depth_scale']\n",
    "        #s_crit = parameter['erode__s_crit']\n",
    "        #string = str(travel_dist) + ',' + str(k_factor) + ',' + str(k_depth) + ',' + str(s_crit)\n",
    "        #with open('all_params.csv','w') as file:\n",
    "        #    file.write(str(self.k_factor) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marine = xs.Model(\n",
    "    {\n",
    "        \"grid\": UniformGrid1D,\n",
    "        \"profile\": ProfileZ,\n",
    "        \"init\": InitBasinGeom,\n",
    "        \"erode\": ErosionDeposition,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marine.visualize(show_inputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to import basement elevation and qs time series after they were exported by the prepro notebook\n",
    "bedrock_file = '../prepro/bedrock_elev_array.npy'\n",
    "br = np.load(bedrock_file)\n",
    "bedrock_elev_array = xr.DataArray(br, dims=['time', 'x'])\n",
    "initial_bedrock = bedrock_elev_array[0, :]\n",
    "\n",
    "qs_file = '../prepro/qs_array.npy'\n",
    "qs_array = np.load(qs_file)\n",
    "qs_array = xr.DataArray(qs_array, dims=['time'])\n",
    "#this is loading in the full m3/yr numbers directly from Baby et al 2019.\n",
    "\n",
    "#so need to divide by the basin width before piping it into the model.\n",
    "#import best-fit basin width from pyABC inference\n",
    "#basin_width_file = '../step1_qs_in/best_fit_basin_width.npy'\n",
    "#basin_width = np.load(basin_width_file)[0] #first and only element of array\n",
    "#qs_array = qs_array[:] / basin_width #now we're in the right units for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ds = xs.create_setup(\n",
    "   ...:     model=marine,\n",
    "   ...:     clocks={\n",
    "   ...:         'time': np.arange(0, 130000000, 1000),\n",
    "   ...:         'otime': np.array([129999000]) #np.array([19999000])\n",
    "   ...:     },\n",
    "   ...:     master_clock='time',\n",
    "   ...:     input_vars={\n",
    "   ...:         'grid': {'length': 740000., 'spacing': 10000.},\n",
    "                'init': {'init_br': initial_bedrock},\n",
    "                'erode': {\n",
    "                    'k_factor': 1.,\n",
    "                    'k_depth_scale': 100.,\n",
    "                    's_crit': 0.1,\n",
    "                    'travel_dist': 10000.,\n",
    "                    'sed_porosity': 0.56,\n",
    "                    'sed_porosity_depth_scale': 2830.,\n",
    "                    'sea_level': 0.,\n",
    "                    'qs_in': qs_array,\n",
    "                    'basin_width': 1.0\n",
    "                },\n",
    "                'profile': {\n",
    "                    'br': bedrock_elev_array\n",
    "                },\n",
    "   },\n",
    "            output_vars={'profile__z': 'otime', 'profile__br': 'otime', 'profile__h': 'otime'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here, I will modify the model definition to be\n",
    "#something like \"run\" fn from my previous inference script\n",
    "def model(parameter):  \n",
    "\n",
    "    model = marine.clone()\n",
    "    #sample = parameter\n",
    "    with model: \n",
    "        #try writing param values out\n",
    "        travel_dist = parameter['erode__travel_dist']\n",
    "        k_factor = parameter['erode__k_factor']\n",
    "        k_depth = parameter['erode__k_depth_scale']\n",
    "        s_crit = parameter['erode__s_crit']\n",
    "        #string = str(travel_dist) + ',' + str(k_factor) + ',' + str(k_depth) + ',' + str(s_crit) + '\\n'\n",
    "        #with open('all_params.csv','a') as file:\n",
    "        #    file.write(string) \n",
    "        start = time.time()\n",
    "        ds_out = (\n",
    "            in_ds\n",
    "            .xsimlab.update_vars(input_vars=parameter)\n",
    "            .xsimlab.run()\n",
    "        )\n",
    "        stop = time.time() - start\n",
    "        print(\"model took\" , stop, \"seconds in total\")\n",
    "    return {\"data\": ds_out.profile__h[-1, 1:], \"params_for_record\":[travel_dist, k_factor, k_depth, s_crit]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We then define the prior for the `mean` to be uniform over the interval $[0, 5]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = pyabc.Distribution(erode__travel_dist=pyabc.RV(\"uniform\", 100000, 200000),\n",
    "                           erode__k_factor=pyabc.RV(\"uniform\", -3, 4),\n",
    "                           erode__k_depth_scale=pyabc.RV(\"uniform\", 1, 199),\n",
    "                           erode__s_crit=pyabc.RV(\"uniform\", 0.01, 0.09))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "(Actually, this has to be read as $[0, 0+5]$. For example, `RV(\"uniform\", 1, 5)` is uniform over the interval $[1,6]$. Check the `scipy.stats` package for details of the definition.)\n",
    "\n",
    "We also need to specify when we consider data to be close in form of a distance funtion.\n",
    "We just take the absolute value of the difference here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x, y): #x is simulated, y is observed\n",
    "    #the first one uses a fixed uncertainty\n",
    "    misfit = np.sqrt((1/73) * np.sum(np.power(y[\"data\"] - x[\"data\"], 2)/np.power(10,2)))\n",
    "    #with open('all_distances.csv','a') as file:\n",
    "    #    file.write(str(np.array(misfit)) + '\\n')\n",
    "        \n",
    "    #write params\n",
    "    params_list = x[\"params_for_record\"]\n",
    "    string = str(params_list[0]) + ',' + str(params_list[1]) + ',' + str(params_list[2]) + ',' + str(params_list[3]) + ',' + str(np.array(misfit)) + '\\n'\n",
    "    with open('all_params_run7.csv','a') as file:\n",
    "        file.write(string) \n",
    "    #the second one uses a fixed percent uncertainty, such that\n",
    "    #thicker piles of sediment have greater uncertainty\n",
    "    #139 is number of comparison points\n",
    "    #0.05 is percentage of observed to call \"measurement uncertainty\"\n",
    "    #misfit = (1/139) * np.sqrt(np.sum(np.power(y[\"data\"] - x[\"data\"], 2)/np.power(0.05 * y[\"data\"],2)))\n",
    "    \n",
    "    return np.double(misfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Now we create the `ABCSMC` object, passing the model, the prior and the distance to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyabc.sampler.Sample(record_rejected=True)\n",
    "sampler = pyabc.sampler.MulticoreEvalParallelSampler(n_procs = 140)\n",
    "abc = pyabc.ABCSMC(model, prior, distance, sampler = sampler,\n",
    "                   population_size = 20)\n",
    "#both of these n_proc numbers can be up to 140 (memory limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyabc.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "To get going, we have to specify where to log the ABC-SMC runs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "We can later query the database with the help of the :class:`History <pyabc.storage.History>` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually you would now have some measure data which you want to know the posterior of.\n",
    "Here, we just assume, that the measured data was 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = (\"sqlite:///\" +\n",
    "           os.path.join(tempfile.gettempdir(), \"test.db\"))\n",
    "\n",
    "#define observation data to match\n",
    "sed_surfaces = np.load('../prepro/all_surfaces.npy')\n",
    "top_surface = sed_surfaces[-1, :] #this is the top of the U8 surface\n",
    "br_surface = sed_surfaces[0, :]\n",
    "data_full_h = top_surface[1:] - br_surface[1:]\n",
    "observation = data_full_h\n",
    "abc.new(db_path, {\"data\": observation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The `new` method returned an integer. This is the id of the ABC-SMC run.\n",
    "This id is only important if more than one ABC-SMC run is stored in the same database.\n",
    "\n",
    "Let's start the sampling now. We'll sample until the acceptance threshold epsilon drops below 0.2. We also specify that we want a maximum number of 10 populations.\n",
    "So whatever is reached first, `minimum_epsilon` or `max_nr_populations`, will stop further sampling.\n",
    "\n",
    "For the simple model we defined above, this should only take a couple of seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_number = 7#used to name results file\n",
    "history = abc.run(minimum_epsilon=0.01, max_nr_populations=15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "The :class:`History <pyabc.storage.History>` object returned by ABCSMC.run can be used to query the database.\n",
    "This object is also available via abc.history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history is abc.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine history\n",
    "plt.figure(figsize=(8,8))\n",
    "return_df = history.get_population_extended(t = \"all\")\n",
    "return_df.to_csv('step2_results_' + str(run_number).zfill(3) + '.csv')\n",
    "\n",
    "sorted_by_fit = return_df.sort_values('distance')\n",
    "sorted_by_fit = sorted_by_fit.iloc[::-1]\n",
    "lambdas = sorted_by_fit[sorted_by_fit['par_name'] == 'erode__travel_dist']['par_val']\n",
    "ks = sorted_by_fit[sorted_by_fit['par_name'] == 'erode__k_factor']['par_val']\n",
    "z0s = sorted_by_fit[sorted_by_fit['par_name'] == 'erode__k_depth_scale']['par_val']\n",
    "misfits = sorted_by_fit[sorted_by_fit['par_name'] == 'erode__travel_dist']['distance']\n",
    "ts = sorted_by_fit[sorted_by_fit['par_name'] == 'erode__travel_dist']['t']\n",
    "plt.scatter(lambdas, np.array(ks) * np.exp(-200/np.array(z0s)), c = np.log(misfits))\n",
    "#sorted_by_fit = return_df.sort_values('distance')\n",
    "#plt.scatter(sorted_by_fit.iloc[3, 7], np.array(sorted_by_fit.iloc[2, 7]) * np.exp(-200/np.array(sorted_by_fit.iloc[1,7])), c = 'r')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel(r'$k*e^{-200/z_*}$')\n",
    "plt.colorbar()\n",
    "plt.ylim(0, 0.05)\n",
    "#for i in range(len(ts)):\n",
    "#    plt.annotate(str(ts.iloc[i]), (lambdas.iloc[i], ks.iloc[i] * np.exp(-200/z0s.iloc[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(np.power(10, np.array(ks)), np.array(z0s), c = np.log(misfits), s=75)\n",
    "#plt.scatter(np.float(sorted_by_fit.iloc[2, 7]), np.float(sorted_by_fit.iloc[1,7]), c = 'r')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(r'$z_*$')\n",
    "plt.colorbar()\n",
    "plt.xlim(0.001, 10)\n",
    "plt.ylim(0, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scs = sorted_by_fit[sorted_by_fit['par_name'] == 'erode__s_crit']['par_val']\n",
    "plt.scatter(lambdas, scs, c = np.log(misfits))\n",
    "#sorted_by_fit = return_df.sort_values('distance')\n",
    "#plt.scatter(sorted_by_fit.iloc[3, 7], np.array(sorted_by_fit.iloc[0, 7]), c = 'r')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel(r'$S_c$')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_fit = return_df.sort_values('distance')\n",
    "print(sorted_by_fit.iloc[0:8, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract and save the best fit params int he following order:\n",
    "#travel distance, k, z*, Sc\n",
    "sorted_by_fit = return_df.sort_values('distance')\n",
    "\n",
    "particle_id_of_best_fit = sorted_by_fit.iloc[0, 4]\n",
    "best_fit_params = sorted_by_fit[sorted_by_fit['particle_id'] == particle_id_of_best_fit]\n",
    "\n",
    "best_fit_travel_dist = best_fit_params['par_val'][best_fit_params['par_name'] == 'erode__travel_dist']\n",
    "best_fit_k = best_fit_params['par_val'][best_fit_params['par_name'] == 'erode__k_factor']\n",
    "best_fit_depth_scale = best_fit_params['par_val'][best_fit_params['par_name'] == 'erode__k_depth_scale']\n",
    "best_fit_s_crit = best_fit_params['par_val'][best_fit_params['par_name'] == 'erode__s_crit']\n",
    "print(np.float(best_fit_s_crit.iloc[0]))\n",
    "best_fit_params = np.array([np.float(best_fit_travel_dist.iloc[0]), \n",
    "                           np.float(best_fit_k.iloc[0]), \n",
    "                           np.float(best_fit_depth_scale.iloc[0]), \n",
    "                           np.float(best_fit_s_crit.iloc[0])]\n",
    "                          )\n",
    "\n",
    "np.save('best_fit_params.npy', best_fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fit = np.where(return_df['distance'] == min(return_df['distance']))[0][0]\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(return_df.sort_values('distance'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "source": [
    "Now we visualize the probability density functions.\n",
    "The vertical line indicates the location of the observation.\n",
    "Given our model, we expect the mean to be close to the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for t in range(history.max_t+1):\n",
    "    df, w = history.get_distribution(m=0, t=t)\n",
    "    pyabc.visualization.plot_kde_1d(\n",
    "        df, w,\n",
    "        xmin=0, xmax=5,\n",
    "        x=\"mean\", ax=ax,\n",
    "        label=\"PDF t={}\".format(t))\n",
    "ax.axvline(observation, color=\"k\", linestyle=\"dashed\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyABC also offers various other visualization routines in order to analyze the parameter estimation run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, arr_ax = plt.subplots(2, 2)\n",
    "\n",
    "pyabc.visualization.plot_sample_numbers(history, ax=arr_ax[0][0])\n",
    "pyabc.visualization.plot_epsilons(history, ax=arr_ax[0][1])\n",
    "pyabc.visualization.plot_credible_intervals(\n",
    "    history, levels=[0.95, 0.9, 0.5], ts=[0, 1, 2, 3, 4],\n",
    "    show_mean=True, show_kde_max_1d=True,\n",
    "    refval={'mean': 2.5}, arr_ax=arr_ax[1][0])\n",
    "pyabc.visualization.plot_effective_sample_sizes(history, ax=arr_ax[1][1])\n",
    "\n",
    "plt.gcf().set_size_inches((12, 8))\n",
    "plt.gcf().tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.all_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "raw_mimetype": "text/restructuredtext"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "all_params = genfromtxt('all_params.csv', delimiter=',')\n",
    "all_distances = genfromtxt('all_distances.csv', delimiter=',')\n",
    "plt.scatter(all_params[:, 0], all_params[:, 3], c = np.log(all_distances), vmin=np.log(8.43), vmax=np.log(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(all_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_output = np.column_stack((all_params,all_distances))\n",
    "all_output=all_output[all_output[:,-1].argsort()[::-1]] #sort by misfit\n",
    "#len(all_output[:-1])\n",
    "print(all_output[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8,8))\n",
    "plt.scatter(all_output[:, 1][::-1], all_output[:, 2][::-1], c = np.log(all_output[:,-1][::-1]), s=75)\n",
    "#plt.scatter(np.float(sorted_by_fit.iloc[2, 7]), np.float(sorted_by_fit.iloc[1,7]), c = 'r')\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel(r'$z_*$')\n",
    "plt.colorbar()\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scs = sorted_by_fit[sorted_by_fit['par_name'] == 'erode__s_crit']['par_val']\n",
    "plt.scatter(all_output[:, 0], all_output[:, 3], c = np.log(all_output[:, -1]), vmin=np.log(8.43), vmax=np.log(20))\n",
    "#plt.scatter(all_output[-1, 0], all_output[-1, 3], c = 'r')\n",
    "plt.scatter(1.76152018e+05, 2.95003540e-02, edgecolors = 'r', marker='o', facecolors='None', s = 100)\n",
    "\n",
    "#sorted_by_fit = return_df.sort_values('distance')\n",
    "#plt.scatter(sorted_by_fit.iloc[3, 7], np.array(sorted_by_fit.iloc[0, 7]), c = 'r')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel(r'$S_c$')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_output[-10000:, :][(all_output[-10000:, 3]>0.028) & (all_output[-10000:, 0]<200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(all_output[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1.07360280e+01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(8.74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:.conda-charlie-env-2]",
   "language": "python",
   "name": "conda-env-.conda-charlie-env-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
